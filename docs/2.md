# Bilder aus einer Seite filtern

## TODO
Parameter

## Installation
Zuerst musst du [beautifulsoup4](https://www.crummy.com/software/BeautifulSoup/#Download) und [requests](http://docs.python-requests.org/en/master/) installieren.
Tippe dazu in einem Terminal deiner Wahl ```(sudo -H) pip3 install beautifulsoup4 requests```.
Öffne die Python3 Konsole und versuche bs4 (beautifulsoup4) und requests zu importieren.
```python
from bs4 import BeautifulSoup
import requests
```
Falls du eine Fehlermeldung kriegst, sind die Pakete nicht installiert. In diesem Fall versuche sie zu installieren, wenn du es nicht gelingt, frage im [Forum](https://the-morpheus.de/forum/index.php) nach.

#### Wozu sind diese Libraries gut?
Beautifulsoup kann HTML Dateien sehr einfach durchsuchen. Du kannst nach Attributen und Tagnamen suchen. Requests erleichtert das anfordern von Webseiten.

## Die Bilder finden
Fangen wir an, indem wir eine ```grab.py``` in unserem Projekt Ordner erstellen.
Dort importieren wir das Objekt BeautifulSoup aus bs4 und requests.
```python
from bs4 import BeautifulSoup
import requests
```
Nun können mit requests.get(url) eine Seite anfordern. Um den Inhalt der Seite zu bekommen greifen wir auf text der Seite zu.
```python
# get page's content
page = requests.get(url).text
print(page)
```
Wenn wir das starten erhalten wir eine Menge Text. Uns interressieren aber nur die Bilder.
Also füttern wir BeautifulSoup mit dem Text.
```python
# find all imgs from page
soup = BeautifulSoup(page, "html.parser")
imgs = soup.find_all("img")
print(img)
```
Das ```"html.parser"``` weißt BeautifulSoup darauf hin diesen Parser zu verwenden. Dann finden wir mit ```soup.find_all("img")``` alle Bilder der Seite.

Jetzt können sie herunterladen. Dafür erstellen wir einen Ordner namens pics in den wir die Bilder speichern können.
```python
# loop through images
i = 0 # image count
for img in imgs:
    # get url the images are hosted on
    src = img.get("src")
    # download and save with image count
    with open("pics/" + str(i) + ".jpg", "wb") as f:
        f.write(requests.get(src).content)
        i += 1 # count
```
Ein Bild hat logischerweise keinen Text, also brauchen wir die Bilder binär.
Deshalb greifen wir auf content statt text zu.
Das dauert eine Weile. Wenn wir den Ordner öffnen sehen wir, das nur die ersten 10 Bilder richtig heruntergeladen wurden.
Das liegt daran, dass die Adresse der folgenden Bilder nicht im src liegt, da diese erst beim scrollen durch die Seite geladen werden. Stattdessen liegt die Adresse in data-lazy.
Dies lässt sich leicht beheben:
```python
# loop through images
i = 0 # image count
for img in imgs:
    # get url the images are hosted on
    src = img.get("src")
    if (src == "/static/img/blank.gif"):
        src = img.get("data-lazy")
    # download and save with image count
    with open("pics/" + str(i) + ".jpg", "wb") as f:
        f.write(requests.get(src).content)
        i += 1 # count
```
Jetzt haben wir alle Bilder heruntergeleden.

Allerdings brauchen wir nur ein zufälliges Bild. Dazu importieren wir choice aus random.
```python
# choose random image from imgs
img = choice(imgs)
# get the url of the img
src = img.get("src")
if src == "/static/img/blank.gif": src = img.get("data-lazy")
```
Choice sucht sich ein zufälliges Element aus einer Liste raus.

Das können wir wieder speichern.
```python
img = requests.get(src).content
with open("img.jpg", "wb") as f:
    f.write(img)
```

### <[:arrow_left: Zurück :arrow_left:](1.md) | [:arrow_right: Weiter :arrow_right:](2.md)>
